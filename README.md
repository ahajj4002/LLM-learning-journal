# LLM-learning-journal
This repository aims at documenting my learning about LLMs: Notes, experiments, and code snippets from my journey studying large language models (LLMs).

Here is the structure I intend to follow:
## Contents
- 'notes/' — topic-based markdown notes (transformers, fine-tuning, etc.)
- 'experiments/' — Python notebooks testing different models and prompts
- 'resources/' — papers, links, and reference material

## Goals
- Build a deep understanding of how LLMs work starting with concepts such as backprogapagtion in neural network, Transformers and its variants
- Experiment with LLM key concepts such as prompt design, fine-tuning, and evaluation
- Create a reference for future projects and research
